---
layout: post
title:  "The (natural?) logarithm"
categories: metrics
draft: true
---

Post about logs.

# Why transform your data?

There are many possible transformations that can be applied to data, such as reciprocals, roots, etc. There are several situations in which it is desirable to apply some non-linear re-expression of the dependent variable.

### Residuals

First, the residuals of your model can have a **skewed distribution**. The idea behind the transformation should be to obtain residuals that are more symmetrically distributed about zero.

Residuals can also present **heteroskedasticity**, which is simply a systematic change in their spread with different values of the independent variables. Analogously, the idea behind the transformation should be to remove that systematic change in spread.

### Linearize relationships

There are models which relationships are not linear, like for example the Cobb-Doublas production function:

$$
Y = A L^\alpha K^{1-\alpha}
$$

You may have **theoretical reasons** to choose that model to fit into your data, so in order to use OLS for instance you have to apply some sort of transformation to your data. Additionally, linear relationships usually can **make estimation easier**, simplify the number or complexity of interaction terms, etc.

## No-nos

While the reasons detailed above are valid from a data-driven perspective, sometimes data scientists use re-expressions for reasons that are not scientifically sound. These include

- **Getting rid of outliers**. Outliers are data points that don't fit a parsimonious, simplified description of the data (a.k.a our model). The presence of outliers should not dictate how we will choose to describe the rest of our data.
- **Getting nicer results**. Don't use it to make "bad" data look well-behaved or show you those pretty significance stars. This is related to the previous point and of course it is not the way to conduct rigorous scientific research.
- **Plotting the data**. If you need a transformation to plot your data, it may be a sign of one of the *good* reasons already mentioned. Or sometimes you just want to show a specific plot with the transformed data, which is fine if it doesn't translate to the actual analysis.
- **Because all the values are positive**. This may call for a different transformation. For instance, [root-transform is known to work best with count data](https://www.r-bloggers.com/do-not-log-transform-count-data-bitches/).

# Why use *logarithmic* transformations?

The general situations described above call for a transformation to better fit your model. When are logarithms preferred?



### Is it that a *natural* logarithm?

Yes it is!

It baffles me that we economists keep saying "take the logarithm of household income" and keep writing $$\log{y}$$, when what we really mean is to take the *natural* logarithm of household income, $$\ln{y}$$. I honestly don't know why this has happened, but in this post (and in general) I will always refer to natural logarithms by their full name and correct notation.

# Is there a reason for preferring natural logarithms in econometrics?

Yes, but the reasons are not extremely strong.

The most commonly cited advantage is that coefficients on the natural-log scale are *directly* interpretable as approximate proportional differences: with a coefficient of 0.07, a difference of 1 in $$x$$ corresponds to an approximate 7% difference in $$y$$ (Gelman and Hill).

Another advantage of natural logs of regular ones is that their first differential is simpler:

$$
\begin{align}
\frac{\ln x}{\partial x} &= \frac{1}{x} \\
\frac{\log x}{\partial x} &= \frac{1}{x\ln{10}}
\end{align}
$$

The emphasis is on *directly* interpretable, because we could take logarithms in a different base and still get similar results.



# References
Andrew Gelman and Jennifer Hill (2007). Data Analysis using Regression and Multilevel/Hierarchical Models. Cambridge University Press: Cambridge; New York, pp. 60-61.
