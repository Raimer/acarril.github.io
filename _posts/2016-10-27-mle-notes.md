---
layout: post
title:  "Some notes on maximum likelihood estimation"
categories: econ R
---

[Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE) methods are widely in econometric analysis. Although the mathematical derivation of what we're doing is clear, I've always wanted to explore the process from a data-driven perspective. In this post I'll go over the intuition of what MLE is doing, using R (and a bit of math) to better understand the underpinnings of this method.

# Basic intuition on maximum likelihood

You have some data and are willing (?) to assume that it is generated from some distribution. Say you assume it comes from a normal (Gaussian) distribution. That's fine, but there are infinitely many possible parameters (i.e. means, variances) that "source" distribution may have:

![Possible normal distributions](/files/convergence_notes-gaussian_dists.png "Which distribution is producing my data?")

The idea behind MLE is to pick the distribution that is "most consistent" with the data. That is, *MLE finds the most likely function that explains the observed data.*

### How can you choose parameters which are "most consistent" with the data?

Say you've got a variable `y=4,1,10`. The most consistent parameters would be to choose a mean equal to 5 and a variance of 14. Yes, some other Gaussian distribution may have produced those values in `y`, but the key is that the probability of getting those particular values of is maximized with the chosen mean and variance.

In a regression framework the mean is simply a linear function of the data. Consider the same vector `y` as before and a new variable `x=1,-1,3`. The mean is the fitted regression model $$X' \hat\beta$$, with $$\hat\beta=[2.75, 2.25]$$. You can easily check this in R:

```R
# Input data
y <- c(4, 1, 10)
x <- c(1, -1, 3)
exampledata <- data.frame(y,x)

# Fit the model
lm(y ~ x, data = exampledata)

##Call:
##lm(formula = y ~ x, data = exampledata)

##Coefficients:
##(Intercept)            x  
##       2.75         2.25
```

# The likelihood function maximization

Let's now expand our example to something a bit more sophisticated. The basic idea still is that we have some points in `x` and `y`, and we want to know the parameters $$\beta$$ and $$\sigma^2$$ that most likely fit the data. We're going to start by cheating and generating the data with known parameters:

```R
# The true parameters
beta <- 2
sigma2 <- 1

# Generate dataset
set.seed(123)
data   <- data.frame(x = runif(100, 1, 10))
data$y <- beta*data$x + rnorm(100, 0, sigma2)
plot(data$x, data$y, xlab="x", ylab="y")
```

### The likelihood function

Suppose we *don't* know the true parameters $$\beta$$ and $$\sigma^2$$ that generate our data (which is usually the case). In MLE linear model, we assume the data points we have come from a normal (Gaussian) distribution with mean $$x \beta$$ and variance $$\sigma^2$$, that is,

$$ y \sim \mathcal{N}(x\beta, \sigma^2) $$

 We know the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function) of that normal distribution is

$$f(x; x\beta, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}\exp{\left(-\frac{(y_i-x_i\beta)^2}{2\sigma^2}\right)}$$

Now, the idea is to find the values for $$\beta$$ and $$\sigma^2$$ that maximize that probability for the points $$(x_i,y_i)$$ in our data. That function is the likelihood function (usually denoted by $$\mathcal{L}$$) and it's simply the product over $$i$$ of the previous expression:

$$ \mathcal{L} = \Pi_{i=1}^n y_i $$

That form of $$\mathcal{L}$$ is a bit unwieldy, so we usually prefer taking logs and maximize the so called log likelihood function:

$$\log(\mathcal{L}) = \sum_{i = 1}^n-\frac{n}{2}\log(2\pi) -\frac{n}{2}\log(\sigma^2) -
      \frac{1}{2\sigma^2}(y_i - x_i\beta)^2$$

That's much better! We can now take this function and write it down in R:

```R
# Log-likelihood function
linear.lik <- function(theta, y, X){
  n      <- nrow(X)
  k      <- ncol(X)
  beta   <- theta[1:k]
  sigma2 <- theta[k+1]
  e      <- y - X%*%beta
  logl   <- -.5*n*log(2*pi) - .5*n*log(sigma2) - ((t(e) %*% e) / (2*sigma2))
  return(-logl)
}
```

### Log-likelihood maximization

Before maximizing $$\log(\mathcal{L})$$, it is cool to see how different values of $$\beta$$ and $$\sigma^2$$ shape its surface. We'll plot it for values of $$\beta$$ and $$\sigma^2$$ going from 0 to 3 in increments of 0.1, using the code below:

```R
# Plot of log-likelihood function for different values of beta and sigma2
surface <- list()
k <- 0
for(beta in seq(0, 3, 0.1)){
  for(sigma2 in seq(0, 3, 0.1)){
    k <- k + 1
    logL <- linear.lik(theta = c(0, beta, sigma2), y = data$y, X = cbind(1, data$x))
    surface[[k]] <- data.frame(beta = beta, sigma2 = sigma2, logL = -logL)
  }
}
surface <- do.call(rbind, surface)
library(lattice)
wireframe(logL ~ beta*sigma2,
          surface,
          colorkey=FALSE,
          drape=TRUE,
          scales = list(arrows = FALSE, z = list(arrows=TRUE)),
          col.regions = rainbow(100, s=1, v=1, start=0, end = max(1,100 - 1)/100, alpha=1))
```

So a pretty neat plot comes out, as you can see below. Where's the maximum? From the plot it is not clear, but there clearly *is* a maximum point. If you squint, you'll probably notice it is somewhere in the ridge along $$\beta=2$$, with $$\sigma^2$$ being less clear... (remember their true values?)

![Plot of likelihood function](/files/mle_max_plot.png "There is a maximum somewhere")

This looks very promising, so we'll now ask R to find the parameters that actually maximize our log-likelihood function:

```R
# Find maximization points
linear.MLE <- nlm(f=linear.lik, p=c(1,1,1), hessian=TRUE, y=data$y, X=cbind(1, data$x))
linear.MLE$estimate
## [1] 0.001049617 1.990013519 0.920734469
```

So we know the true parameters of the distribution are $$\beta=2$$ and $$\sigma^2=1$$. We notice that the maximization process yields a pretty accurate estimate for $$\beta$$ and $$\sigma^2$$. Not bad at all! The first value is also very accurate: this corresponds to the intercept, which we have implicitly to be equal to zero throughout this example.
