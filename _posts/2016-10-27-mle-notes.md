---
layout: post
title:  "Some notes on maximum likelihood estimation"
categories: stata
---

[Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (MLE) methods are widely in econometric analysis. Although the mathematical derivation of what we're doing is clear, I've always wanted to explore the process from a data-driven perspective. In this post I'll go over the intuition of what MLE is doing, using R and Stata (and a bit of math) to better understand the underpinnings of this method.

# Basic intuition on maximum likelihood

You have some data and are willing (?) to assume that it is generated from some distribution. Say you assume it comes from a normal (Gaussian) distribution. That's fine, but there are infinitely many possible parameters (i.e. means, variances) that "source" distribution may have:

![Possible normal distributions](/files/convergence_notes-gaussian_dists.png "Which distribution is producing my data?")

The idea behind MLE is to pick the distribution that is "most consistent" with the data. That is, MLE finds the most likely function that explains the observed data.

### How can you choose parameters which are "most consistent" with the data?

Say you've got a variable `y=4,1,10`. The most consistent parameters would be to choose a mean equal to 5 and a variance of 14. Yes, some other Gaussian distribution may have produced those values in `y`, but the key is that the probability of getting those particular values of is maximized with the chosen mean and variance.

In a regression framework the mean is simply a linear function of the data. Consider the same vector `y` as before and a new variable `x=1,-1,3`. The mean is the fitted regression model $$X' \hat\beta$$, with $$\hat\beta=[2.75, 2.25]$$. You can easily check this in Stata:

```
set obs 3

gen y = .
replace y = 4 in 1
replace y = 1 in 2
replace y = 10 in 3

gen x = .
replace x = 1 in 1
replace x = -1 in 2
replace x = 3 in 3

reg y x
```

## The likelihood function maximization

Let's now expand our example to something a bit more sophisticated. The basic idea still is that we have some points in `x` and `y`, and we want to know the parameters $$\mu$$ and $$\sigma$$ that most likely fit the data. We're going to start by cheating and generating the data with known parameters:

```Rscript
# The true parameters
beta <- 1.5
sigma2 <- 0.5

# Generate dataset
data   <- data.frame(x = runif(300, 1, 10))
data$y <- 0 + 0.5*beta*data$x + rnorm(300, 0, sigma2)
plot(data$x, data$y)
```

continues...

![Plot of likelihood function](/files/mle_max_plot.png "There is a maximum somewhere")

#Â A bit of math
We know that the maximization problem is

$$ L(\hat{\boldsymbol{\theta}};\boldsymbol{Z}) = \max_{\boldsymbol{t}\in\Theta} L(\boldsymbol{t};\boldsymbol{Z}) $$

which simply means we are maximizing ...

# References
William Gould, Jeffrey Pitblado, and Brian Poi. 2010.  *Maximum Likelihood Estimation with Stata*, Fourth Edition. Stata Press.
